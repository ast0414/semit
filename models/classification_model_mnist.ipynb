{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model for MNIST data\n",
    "\n",
    "Used the following repos/articles for guidance and code:\n",
    "1. https://github.com/astorfi/TensorFlow-World/tree/master/docs/tutorials/3-neural_network/autoencoder\n",
    "2. https://towardsdatascience.com/understanding-input-and-output-shapes-in-convolution-network-keras-f143923d56ca\n",
    "3. https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d\n",
    "4. https://paperswithcode.com/sota/image-classification-on-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import transform\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "# keras API uses TensorFlow or Theano\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Reshape, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_full.pkl\n",
      "obtained mnist training and testing files\n"
     ]
    }
   ],
   "source": [
    "file_pathway = 'mnist_full.pkl' # found in data folder\n",
    "print(file_pathway)\n",
    "with open(file_pathway, 'rb') as f:\n",
    "    mnist_full = pickle.load(f)\n",
    "    mnist_x_train = mnist_full[\"x_train\"]\n",
    "    mnist_y_train = mnist_full[\"y_train\"]\n",
    "    mnist_x_test = mnist_full['x_test']\n",
    "    mnist_y_test = mnist_full['y_test']\n",
    "\n",
    "print(\"obtained mnist training and testing files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(55000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# greyscale visualization\n",
    "#(x_trn, y_trn), (x_tst, y_tst) = tf.keras.datasets.mnist.load_data() --> keras data itself\n",
    "x_trn = mnist_x_train\n",
    "y_trn = mnist_y_train\n",
    "x_tst = mnist_x_test\n",
    "y_tst = mnist_y_test\n",
    "img_idx = 77 # change this to be any index up to 60,000 as that is the size of the trained set\n",
    "print(y_trn[img_idx])\n",
    "print(x_trn.shape)\n",
    "#plt.imshow(x_trn[img_idx], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_batch(imgs):\n",
    "    # A function to resize a batch of MNIST images to (32, 32)\n",
    "    # Args:\n",
    "    #   imgs: a numpy array of size [batch_size, 28 X 28].\n",
    "    # Returns:\n",
    "    #   a numpy array of size [batch_size, 32, 32].\n",
    "    imgs = imgs.reshape((-1, 28, 28, 1))\n",
    "    resized_imgs = np.zeros((imgs.shape[0], 32, 32, 1))\n",
    "    for i in range(imgs.shape[0]):\n",
    "        resized_imgs[i, ..., 0] = transform.resize(imgs[i, ..., 0], (32, 32))\n",
    "    return resized_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaping\n",
      "og x_trn.shape:  (55000, 1, 28, 28)\n",
      "og x_tst.shape:  (10000, 1, 28, 28)\n",
      "x_trn_resize_32.shape:  (55000, 32, 32, 1)\n",
      "x_tst_resize_32.shape:  (10000, 32, 32, 1)\n",
      "new x train shape:  (55000, 32, 32, 1)\n",
      "Number of images in x train:  55000\n",
      "Number of images in x test:  10000\n",
      "finished reshaping\n"
     ]
    }
   ],
   "source": [
    "# reshape and normalize images\n",
    "\n",
    "print(\"reshaping\")\n",
    "\n",
    "pix_size = 32\n",
    "\n",
    "print(\"og x_trn.shape: \",x_trn.shape)\n",
    "print(\"og x_tst.shape: \", x_tst.shape)\n",
    "\n",
    "x_trn_resize_32 = resize_batch(x_trn)\n",
    "x_tst_resize_32 = resize_batch(x_tst)\n",
    "\n",
    "print(\"x_trn_resize_32.shape: \",x_trn_resize_32.shape)\n",
    "print(\"x_tst_resize_32.shape: \",x_tst_resize_32.shape)\n",
    "\n",
    "x_trn_2 = x_trn_resize_32.reshape(x_trn_resize_32.shape[0], pix_size, pix_size, 1)\n",
    "x_tst_2 = x_tst_resize_32.reshape(x_tst_resize_32.shape[0], pix_size, pix_size, 1)\n",
    "x_trn_2 = x_trn_2.astype('float32')\n",
    "x_tst_2 = x_tst_2.astype('float32')\n",
    "\n",
    "x_trn_2 /= 255\n",
    "x_tst_2 /= 255\n",
    "\n",
    "print(\"new x train shape: \", x_trn_2.shape)\n",
    "print(\"Number of images in x train: \", x_trn_2.shape[0])\n",
    "print(\"Number of images in x test: \", x_tst_2.shape[0])\n",
    "\n",
    "print(\"finished reshaping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building convolutional neural network -- CNN\n",
    "in_shape = (32, 32, 1)\n",
    "mdl = Sequential()\n",
    "kernel_size_conv = (4,4) # originally 3,3\n",
    "#convolutional layer\n",
    "conv_2d = Conv2D(pix_size, kernel_size=kernel_size_conv, padding=\"same\", strides=(2,2), input_shape=in_shape)\n",
    "mdl.add(conv_2d)\n",
    "\n",
    "#increasing output channel (filter) size\n",
    "in_shape = (32, 32, 1)\n",
    "conv_2d = Conv2D(64, kernel_size=kernel_size_conv, padding=\"same\", strides=(2,2), input_shape=in_shape)\n",
    "mdl.add(conv_2d)\n",
    "in_shape = (64, 64, 1)\n",
    "conv_2d = Conv2D(128, kernel_size=kernel_size_conv, padding=\"same\", strides=(2,2), input_shape=in_shape)\n",
    "mdl.add(conv_2d)\n",
    "in_shape = (128, 128, 1)\n",
    "conv_2d = Conv2D(256, kernel_size=kernel_size_conv, padding=\"same\", strides=(2,2), input_shape=in_shape)\n",
    "mdl.add(conv_2d)\n",
    "in_shape = (256, 256, 1)\n",
    "conv_2d = Conv2D(512, kernel_size=kernel_size_conv, padding=\"same\", strides=(2,2), input_shape=in_shape)\n",
    "mdl.add(conv_2d)\n",
    "\n",
    "#leaky relu layer\n",
    "leaky_reLU = LeakyReLU(alpha=0.2)\n",
    "mdl.add(leaky_reLU)\n",
    "#maxpool_2d = MaxPooling2D(pool_size=(2,2))\n",
    "\n",
    "#dropout layer\n",
    "dropout = Dropout(0.2)\n",
    "mdl.add(dropout)\n",
    "\n",
    "#flatten layer\n",
    "flat = Flatten()\n",
    "mdl.add(flat)\n",
    "\n",
    "#dense layer\n",
    "dense_r = 512 # originally 128\n",
    "denselayer_r = Dense(dense_r, activation=tf.nn.relu) # changed density\n",
    "mdl.add(denselayer_r)\n",
    "dense_c = 10\n",
    "denselayer_c = Dense(dense_c, activation=tf.nn.softmax)\n",
    "mdl.add(denselayer_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 229s 4ms/step - loss: 0.4201 - accuracy: 0.8595\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 227s 4ms/step - loss: 0.1791 - accuracy: 0.9453\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 227s 4ms/step - loss: 0.1486 - accuracy: 0.9533\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 385s 7ms/step - loss: 0.1311 - accuracy: 0.9598\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 226s 4ms/step - loss: 0.1231 - accuracy: 0.9627\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 226s 4ms/step - loss: 0.1093 - accuracy: 0.9670\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 241s 4ms/step - loss: 0.1082 - accuracy: 0.9672\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 226s 4ms/step - loss: 0.1167 - accuracy: 0.9656\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 311s 6ms/step - loss: 0.0934 - accuracy: 0.9722\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 231s 4ms/step - loss: 0.0942 - accuracy: 0.9718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x152c85d68>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compiling and fitting model\n",
    "\n",
    "mdl.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "mdl.fit(x=x_trn_2, y=y_trn, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 159us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11381538770179468, 0.9672999978065491]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate model\n",
    "mdl.evaluate(x_tst_2, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#individual predictions\n",
    "img_idx = 4444\n",
    "plt.imshow(x_tst_2[img_idx].reshape(pix_size,pix_size), cmap='Greys')\n",
    "pred = mdl.predict(x_tst_2[img_idx].reshape(1, pix_size, pix_size, 1))\n",
    "print(pred.argmax())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
